<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning table-top manipulation tasks using goal images generated by pre-trained text-to-image models">
  <meta name="keywords" content="Visual Reinforcement Learning, Diffusion Models, Text-to-Image Editing, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LfVoid: Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LfVoid: Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://gaojl19.github.io/">Jialu Gao*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hukz18.github.io/">Kaizhe Hu*</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://xugw-kevin.github.io/">GuoWei Xu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://hxu.rocks/">Huazhe Xu</a><sup>1,2,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Qi Zhi Institute,</span>
            <span class="author-block"><sup>3</sup>Shanghai AI Lab</span>
          </div>
          <p class="is-size-7 is-text-centered">
            *Denotes equal contribution
            <br><br>
          </p>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Neurips 2023</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.07837.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.07837"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code Coming Soon</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="hero-body">
      <div class="columns is-centered has-text-justified">
        <div class="column is-8">
          <img id="teaser" height="100%" src="./static/images/Pipeline.png" alt="Teaser">
          <h2 class="caption is-size-5 subtitle has-text-centered">
            LfVoid can generate goal images using image editing methods and learn table-top manipulation tasks with visual reinforcement learning.
          </h2>
        </div>
      </div>
    </div>
  </section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            Pre-trained text-to-image generative models can produce diverse, semantically rich,
            and realistic images from natural language descriptions. Compared with language,
            images usually convey information with more details and less ambiguity. In this
            study, we propose Learning from the Void (LfVoid), a method that leverages the
            power of pre-trained text-to-image models and advanced image editing techniques
            to guide robot learning. Given natural language instructions, LfVoid can edit the
            original observations to obtain goal images, such as “wiping” a stain off a table.
            Subsequently, LfVoid trains an ensembled goal discriminator on the generated
            image to provide reward signals for a reinforcement learning agent, guiding it
            to achieve the goal. The ability of LfVoid to learn with zero in-domain training
            on expert demonstrations or true goal observations (the void) is attributed to the
            utilization of knowledge from web-scale generative models. We evaluate LfVoid
            across three simulated tasks and validate its feasibility in the corresponding real
            world scenarios. In addition, we offer insights into the key considerations for the
            effective integration of visual generative models into robot learning workflows.
            We posit that our work represents an initial step towards the broader application
            of pre-trained visual generative models in the robotics field.
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Methods. -->
      <div class="columns is-centered has-text-justified">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
            <img id="teaser" height="100%" src="./static/images/OverView.png" alt="Teaser">
            LfVoid consists of two parts: (a) Goal image generation and (b) Example-Based Visual RL.<br>
            (a) Goal image generation applies image editing on the initial observations according 
            to different editing instructions to obtain a visual goal dataset. <br>
            (b) Example-Based Visual RLperform reinforcement learning on the 
            generated dataset to achieve the desired goal image in various environments. <br>
        </div>
      </div>
    </div>
</section>

<section class="section">
    <div class="container">
      <div class="columns is-centered has-text-justified">
        <div class="column is-four-fifths">
          <h2 class="title is-3">  Goal Generation Results</h2>
          <img height="80%" src="./static/images/Appearance Based Editing.png">
          Goal generation results of LfVoid on appearance-based editing. The Wipe task and LED task require changing the appearance of an object, such as the surface of a table and the color of a LED light.
        <br>
        <br>
        <br>

        <img height="80%" src="./static/images/Structure Based Editing.png">
          Goal generation results of LfVoid on structure-based editing. The Push task requires changing the structure of an image, such as relocating the red cube from the center of an image to the corner.
        <br>
        <br>
        <br>

        <img height="60%" src="./static/images/General Editing.png">
          General editing results of LfVoid. We report the performance of LfVoid on general editing tasks and show that LfVoid can better perform localized edits and preserve the background of the original image.
        <br>
        <br>
        <br>

          <img height="100%" src="./static/images/Additional Editing Results.png">
		    Additional results of goal images generated by LfVoid.  
        <br>
        <br>
        <br>

        </div>
      </div>
    </div>
</section>


<section class="section">
    <div class="container">
      <div class="columns is-centered has-text-justified">
        <div class="column is-four-fifths">
          <h2 class="title is-3">  Visual Reinforcement Learning Results</h2>
            <img height="80%" src="./static/images/sim rl.png">
            The episode reward curve of simulation tasks. We show the results of the CLIP baseline (CLIP), 
            InstructPix2Pix baseline (IP2P), using real goal image as an upper bound (Real Goal), and LfVoid (Ours)
            <br>
            <br>
            <br>

            <img height="80%" src="./static/images/sim rl numeric.png">
            The numerical metrics of simulation tasks. We report the success rate for LED and Push, and
            the number of stain patches cleaned for Wipe.
            <br>
            <br>
            <br>

            <img height="80%" src="./static/images/real rl.png">
                Visualization of the reward function on Real Robot environments. We visualize the classifier-based reward function 
                obtained by the goal images generated through LfVoid (Ours) and other baselines on successful trajectories. 
                Results show that the reward function obtained by LfVoid can provide near monotonic dense signals, comparable to those from the real goals.
            <br>
            <br>
            <br>

            <p> Additionally, we provide some visualization videos of the classifier-based reward functions obtained by LfVoid on
                several successful and failed trajectories. We can observe that our reward function can assign monotonic increasing values
                for the successful demonstrations, while for failure trajectories the reward curve is almost flat. 
                These visualizations further demonstrate LfVoid's plausibility for real world robotic tasks.</p>
            <br>
            <br>

            <table>
                <thead>
                <tr>
                    <th></th>
                    <th></th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>
                    <figure class="small text-center"><img
                        src="./static/gifs/push_success.gif"
                        alt="Push Success">
                        <figcaption>
                        <p>Push Success</p>
                        </figcaption>
                    </figure>
                    </td>
                    <td>
                    <figure><img
                        src="./static/gifs/push_failure.gif"
                        alt="Push Failure">
                        <figcaption>
                        <p>Push Failure</p>
                        </figcaption>
                    </figure>
                    </td>
                </tr>
                <tr>
                    <td>
                    <figure class="small text-center"><img
                        src="./static/gifs/led_success.gif"
                        alt="LED Success">
                        <figcaption>
                        <p>LED Success</p>
                        </figcaption>
                    </figure>
                    </td>
                    <td>
                    <figure><img
                        src="./static/gifs/led_failure.gif"
                        alt="LED Failure">
                        <figcaption>
                        <p>LED Failure</p>
                        </figcaption>
                    </figure>
                    </td>
                </tr>
                <tr>
                    <td>
                    <figure class="small text-center"><img
                        src="./static/gifs/wipe_success.gif"
                        alt="Wipe Success">
                        <figcaption>
                        <p>Wipe Success</p>
                        </figcaption>
                    </figure>
                    </td>
                    <td>
                    <figure><img
                        src="./static/gifs/wipe_failure.gif"
                        alt="Wipe Failure">
                        <figcaption>
                        <p>Wipe Failure</p>
                        </figcaption>
                    </figure>
                    </td>
                </tr>
                </tbody>
            </table>

        </div>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container content">
      <div class="columns is-centered">
        <div class="column is-two-thirds has-text-justified">
          <h2 class="title">BibTeX</h2>
          <pre><code>@misc{gao2023pretrained,
            title={Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?}, 
            author={Jialu Gao and Kaizhe Hu and Guowei Xu and Huazhe Xu},
            year={2023},
            eprint={2307.07837},
            archivePrefix={arXiv},
            primaryClass={cs.RO}
      }
</code></pre>
        </div>
      </div>
    </div>
  </section>


<section class="section" id="acknowledgements">
    <div class="container content">
      <div class="columns is-centered">
        <div class="column is-two-thirds has-text-justified">
          <h2 class="title">Acknowledgements</h2>
          This work is supported by National Key R&D Program of China (2022ZD0161700).
          <p>Website adapted from the following <a href="http://nerfies.github.io">template</a>.</p>
        </div>
      </div>
    </div>
  </section>

</body>
</html>