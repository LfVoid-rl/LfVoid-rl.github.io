<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning? | LfVoid</title>
  <meta name="viewport" content="width=device-width,minimum-scale=1">
  <meta name="description"
    content="LfVoid Abstract Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as “wiping” a stain off a table.">
  <meta name="generator" content="Hugo 0.111.3">




  <meta name="robots" content="noindex, nofollow">



  <link rel="stylesheet"
    href="static/main.min.css">
  <meta property="og:title"
    content="Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?">
  <meta property="og:description"
    content="LfVoid Abstract Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as “wiping” a stain off a table.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="http://localhost:1313/posts/lfvoid/">
  <meta property="article:section" content="posts">
  <meta property="article:published_time" content="2023-05-18T16:47:08+08:00">
  <meta property="article:modified_time" content="2023-05-18T16:47:08+08:00">
  <meta itemprop="name"
    content="Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?">
  <meta itemprop="description"
    content="LfVoid Abstract Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as “wiping” a stain off a table.">
  <meta itemprop="datePublished" content="2023-05-18T16:47:08+08:00">
  <meta itemprop="dateModified" content="2023-05-18T16:47:08+08:00">
  <meta itemprop="wordCount" content="313">
  <meta itemprop="keywords" content="">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title"
    content="Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?">
  <meta name="twitter:description"
    content="LfVoid Abstract Pre-trained text-to-image generative models can produce diverse, semantically rich, and realistic images from natural language descriptions. Compared with language, images usually convey information with more details and less ambiguity. In this study, we propose Learning from the Void (LfVoid), a method that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide robot learning. Given natural language instructions, LfVoid can edit the original observations to obtain goal images, such as “wiping” a stain off a table.">


  <style type="text/css">
    .fc-ab-root {
      display: none !important
    }

    body>div.fc-ab-root {
      display: none !important
    }

    .fbs-auth__container.fbs-auth__adblock {
      display: none !important
    }

    .overlay-34_Kj {
      display: none !important
    }

    .wrapper-3AzfF {
      display: none !important
    }

    .fEy1Z2XT {
      display: none !important
    }

    .nytc---modal-window---windowContainer.nytc---modal-window---isShown.nytc---shared---blackBG {
      display: none !important
    }

    .tp-modal {
      display: none !important
    }

    .tp-backdrop.tp-active {
      display: none !important
    }

    .c-nudge__container.c-gate__container {
      display: none !important
    }

    .c-nudge__container.c-regGate__container {
      display: none !important
    }

    .css-n7r8pg {
      display: none !important
    }

    .css-1bd8bfl {
      display: none !important
    }

    .overlay__59af11e2 {
      display: none !important
    }

    .tp_modal {
      display: none !important
    }

    .tp-backdrop.tp-active {
      display: none !important
    }

    div[class^='sp_message_container'] {
      display: none !important
    }

    div[class^='sp_veil'] {
      display: none !important
    }
  </style>
</head>

<body class="ma0 avenir bg-near-white">





  <header hmtoqi94wc1ob524r8v24="">
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
        <div class="flex-l justify-between items-center center">
          <a href="http://localhost:1313/" class="f3 fw2 hover-white no-underline white-90 dib">

            LfVoid

          </a>
          <div class="flex-l items-center">




            <div class="ananke-socials">

            </div>

          </div>
        </div>
      </nav>

    </div>
  </header>



  <main class="pb7" role="main" hmtoqi94wc1ob524r8v24="">


    <article class="flex-l flex-wrap justify-between mw8 center ph3 post-content">
      <header class="mt4 w-100">











        <div id="sharing" class="mt3 ananke-socials">

        </div>


        <h1 class="f1 athelas mt3 mb1">LfVoid: Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement
          Learning?</h1>







      </header>
      <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100">
        <h2 id="abstract">Abstract</h2>
        <p>Pre-trained text-to-image generative models can produce <strong>diverse</strong>, semantically
          <strong>rich</strong>, and <strong>realistic</strong> images from natural language descriptions. Compared with
          language, images usually convey information with more <strong>details</strong> and <strong>less
            ambiguity</strong>. In this study, we propose <strong>Learning from the Void (LfVoid)</strong>, a method
          that leverages the power of pre-trained text-to-image models and advanced image editing techniques to guide
          robot learning. Given natural language instructions, LfVoid can <strong>edit</strong> the original
          observations to obtain goal images, such as “wiping” a stain off a table. Subsequently, LfVoid trains an
          ensembled goal discriminator on the generated image to provide reward signals for a reinforcement learning
          agent, guiding it to <strong>achieve</strong> the goal. The ability of LfVoid to learn with
          <strong>zero</strong> in-domain training on expert demonstrations or true goal observations <strong>(the
            void)</strong> is attributed to the utilization of knowledge from web-scale generative models. We evaluate
          LfVoid across three simulated tasks and validate its feasibility in the corresponding real-world scenarios. In
          addition, we offer insights into the key considerations for the effective integration of visual generative
          models into robot learning workflows. We posit that our work represents an initial step towards the broader
          application of pre-trained visual generative models in the robotics field.</p>
        <h2 id="figures">Figures</h2>
        <p><img
            src="static/OverView.png"
            alt="Overview"></p>
        <h2 id="videos">Videos</h2>
        <p>In this section, we should some videoes of the reward curve obtained from trained classifiers of LfV for some
          success and failure trajectories. Those classifiers use the edited images of LfV as positive samples, and
          observations in the replay buffer as negative samples. For each trajectory, the reward and the aligned
          observation at each timestep is provided.</p>
        <p>From the visualization, it’s easy to see that our classifier can assign a monotonic increasing reward curve
          for those successful demonstrations, will the reward curve for failure trajectories is almost flat till the
          end. The videoes demonstrate LfV’s plausibility for real world robotic tasks.</p>
        <table style="border-spacing:0em">
          <thead>
            <tr>
              <th></th>
              <th></th>
              <th></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>
                <figure><img
                    src="static/push_reward_success.gif"
                    width="200%">
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/led_reward_success.gif">
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/wipe_reward_success.gif">
                </figure>
              </td>
            </tr>
            <tr>
              <td>
                <figure><img
                    src="static/push_agent_success.gif"
                    alt="Push Success">
                  <figcaption>
                    <p>Push Success</p>
                  </figcaption>
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/led_agent_success.gif"
                    alt="LED Success">
                  <figcaption>
                    <p>LED Success</p>
                  </figcaption>
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/wipe_agent_success.gif"
                    alt="Wipe Success">
                  <figcaption>
                    <p>Wipe Success</p>
                  </figcaption>
                </figure>
              </td>
            </tr>
          </tbody>
        </table>
        <table>
          <thead>
            <tr>
              <th></th>
              <th></th>
              <th></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>
                <figure><img
                    src="static/push_reward_fail.gif"
                    width="200%">
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/led_reward_fail.gif">
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/wipe_reward_fail.gif">
                </figure>
              </td>
            </tr>
            <tr>
              <td>
                <figure><img
                    src="static/push_agent_fail.gif"
                    alt="Push Failure">
                  <figcaption>
                    <p>Push Failure</p>
                  </figcaption>
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/led_agent_fail.gif"
                    alt="LED Failure">
                  <figcaption>
                    <p>LED Failure</p>
                  </figcaption>
                </figure>
              </td>
              <td>
                <figure><img
                    src="static/wipe_agent_fail.gif"
                    alt="Wipe Failure">
                  <figcaption>
                    <p>Wipe Failure</p>
                  </figcaption>
                </figure>
              </td>
            </tr>
          </tbody>
        </table>
        <h2 id="paper">Paper</h2>

        <embed src="static/LfVoid.pdf" type="application/pdf" width="100%" height="1200px" />

        <ul class="pa0">

        </ul>
        <div class="mt6 instapaper_ignoref">


        </div>
      </div>

      <aside class="w-30-l mt6-l">




      </aside>

    </article>

  </main>
  <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo" hmtoqi94wc1ob524r8v24="">
    <div class="flex justify-between">
      <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/">
        © LfVoid 2023
      </a>
      <div>
        <div class="ananke-socials">

        </div>
      </div>
    </div>
  </footer>



</body>

</html>
